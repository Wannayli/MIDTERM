{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcTQVLmFwDQepcMvFF/lvZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wannayli/MIDTERM/blob/main/Spark_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the environment"
      ],
      "metadata": {
        "id": "QZc4grG8g_n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java --version\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ode5718hCRD",
        "outputId": "0b5d8adb-d17a-4db1-ed1f-e2482bad8181"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.18 2023-01-17\n",
            "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "Python 3.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOLKmypMhENA",
        "outputId": "148a8973-5eea-41c9-a28b-0de8baca4f01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=dc3112a1b02872ff9094061c73e2efcc59b7c93685d52418d4cf5da001c025f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Spark session"
      ],
      "metadata": {
        "id": "vP2H6LW1gj_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredNetworkWordCount\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "PODXD6nVgwW_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To allow automatic schemaInference while reading\n",
        "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
        "\n",
        "# Create the streaming_df to read from input directory\n",
        "streaming_df = spark.readStream\\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"cleanSource\", \"archive\") \\\n",
        "    .option(\"sourceArchiveDir\", \"data/archive/device_data/\") \\\n",
        "    .option(\"maxFilesPerTrigger\", 1) \\\n",
        "    .load(\"sample_data/news_data\")"
      ],
      "metadata": {
        "id": "Vkf7aOnsnKOL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Place the json file"
      ],
      "metadata": {
        "id": "5Ss6hDXioZMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Node.js program to demonstrate the \n",
        "# fs.createWriteStream() method \n",
        "  \n",
        "// Include fs module \n",
        "let fs = require('fs'), \n",
        "fs.createWriteStream( path, options )\n",
        "\n",
        "writeStream.start();\n",
        "FileSource[sample_data/news_data]\n",
        "\n",
        "# To the schema of the data, place a sample json file and change readStream to read \n",
        "streaming_df.printSchema()\n",
        "streaming_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "XOBbmrJUoXWr",
        "outputId": "b3a3ae97-6a6e-40bb-9ba5-982e7ab4e550"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-59bb3f82a94c>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    // Include fs module\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets explode the data as devices contains list/array of device reading\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "exploded_df = streaming_df \\\n",
        "    .select(\"customerId\", \"eventId\", \"eventOffset\", \"eventPublisher\", \"eventTime\", \"data\") \\\n",
        "    .withColumn(\"devices\", explode(\"data.devices\")) \\\n",
        "    .drop(\"data\")\n"
      ],
      "metadata": {
        "id": "mSrN0yydmxpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening dataframe"
      ],
      "metadata": {
        "id": "7Dd0T1ttoli7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the exploded df\n",
        "flattened_df = exploded_df \\\n",
        "    .selectExpr(\"link\", \"headline\", \"category\", \"short_description\", \"authors\", \n",
        "                \"date\") "
      ],
      "metadata": {
        "id": "OdKQNVZroWlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write streaming data to a output folder"
      ],
      "metadata": {
        "id": "z8l6jQlUpMpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the output to console sink to check the output\n",
        "writing_df = flattened_df.writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"path\", \"data/output-5999222020/device_data\") \\\n",
        "    .option(\"checkpointLocation\",\"checkpoint_dir\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "    \n",
        "# Start the streaming application to run until the following happens\n",
        "# 1. Exception in the running program\n",
        "# 2. Manual Interruption\n",
        "writing_df.awaitTermination()"
      ],
      "metadata": {
        "id": "QllSzAezpSSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output"
      ],
      "metadata": {
        "id": "bzwms-HQsFEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data at the output location\n",
        "\n",
        "out_df = spark.read.json(\"data/output-5999222020/news_data/\")\n",
        "out_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "mqV_ChoFsCZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}